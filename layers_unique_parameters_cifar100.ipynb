{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6lt8Ux3pdY0b"},"outputs":[],"source":["# importing libraries\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","# %matplotlib inline\n","import time\n","import torch\n","import torchvision\n","import tarfile\n","from torchvision.datasets.utils import download_url\n","from torch.utils.data import random_split\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as tt\n","from torchvision.utils import make_grid\n","\n","# --- \n","\n","\"\"\"We'll download the images in PNG format from [this page](https://course.fast.ai/datasets), using some helper functions from the `torchvision` and `tarfile` packages.\n","#..............................our part...................................#\n","\n","function  x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))   https://www.desmos.com/calculator/0hiyjyun2f (move points to play with the parameters)\n","\n","\"\"\"\n","#globals \n","#----------------------------------------------------------------------------#\n","L=0\n","Q=0\n","P=0\n","Z=0\n","I=0\n","#----------------------------------------------------------------------------#\n","\n","\n","setAll=False# will train and predict useing all the data ignoring the 2 above\n","#if false set the data manulaiy \n","#-----------------------------------------#\n","val_percent=0.5#x*5000 = dataset\n","train_percent=0.01#x*50000 = dataset\n","\n","#old method:\n","# train_running_size=1000#train size of data {max is 50000}\n","# val_running_Size=1000#val size of data {max is 5000}\n","\n","#-----------------------------------------#\n","\n","\n","\n","#Relu parameters to compare\n","#-----------------------------------------#\n","addMaxRelu=True\n","addMean=True\n","HowManyTimesReRunRelu=10\n","#-----------------------------------------#\n","\n","\n","\n","\n","checkCertainParams=False#if the one above is true set the parameters u want to check, otherwise it will igonre the value:\n","#-----------------------------------------------------------------------------------------#\n","howManyTimeToCheckTheCertainParams=10# how many times to run the cetrtain params \n","addMaxToCertain=True#will return the max score of the certain params\n","addMinToCertain=True#will return the min score of the certain params\n","addMeanToCertain=True#will return the mean of all the score's of the certain params\n","#-----------------------------------------------------------------------------------------#\n","\n","#Layers const parameters for dynamic function, each number represent a diffrent layer:\n","#-----------------------------------------------------------------------------------------#\n","#layer 1 values \n","L1=18.803015465431965\n","Q1=1.7782794100389232\n","P1=2\n","Z1=10\n","I1=1\n","\n","#layer 2 values \n","L2=7.071067811865475\n","Q2=1.7782794100389232\n","P2=P\n","Z2=Z\n","I2=1\n","\n","\n","#layer 3 values \n","L3=13.572088082974531\n","Q3=0.1\n","P3=P\n","Z3=Z\n","I3=-1\n","\n","#layer 4 values \n","L4=3.6840314986403864\n","Q4=0.1\n","P4=P\n","Z4=Z\n","I4=-1.4422495703074083\n","\n","#layer 5 values \n","L5=3.6840314986403864\n","Q5=1\n","P5=P\n","Z5=Z\n","I5=2.080083823051904\n","\n","#layer 6 values \n","L6=2.6591479484724942\n","Q6=0.05623413251903491\n","P6=P\n","Z6=Z\n","I6=1.4422495703074083\n","\n","#layer 7 values \n","L7=7.071067811865475\n","Q7=0.05623413251903491\n","P7=P\n","Z7=Z\n","I7=1\n","\n","#layer 8 values \n","L8=0\n","Q8=0\n","P8=P\n","Z8=Z\n","I8=0\n","#-----------------------------------------------------------------------------------------#\n","\n","\n","\n","#if checkCertainParams is false, config the range you'd like to check\n","#-----------------------------------------------------------------------------------------#\n","LogarithmINC=True#insted of incresing when checking parameters with a constant , will will get to the to value useing geometric progression\n","SIZEI=3#how many i to check\n","SIZEL=4#how many l to check\n","SIZEQ=4#how many q to check\n","#what number the paramter start and end\n","fromI=-1\n","toI=-3\n","\n","fromL=1\n","toL=50\n","\n","fromQ=0.01\n","toQ=10\n","#size of splits will, example fromI=1 toI=3,SIZEI=3 then i will check 1,2,3\n","#-----------------------------------------------------------------------------------------#\n","\n","\n","\n","#Layers SetDynamic:# else the layers parameters for the function will be the one above (when l is 0 then dynamic = relu)\n","#-----------------------------------------------------------------------------------------#\n","DynamicLayer1=False\n","DynamicLayer2=False\n","DynamicLayer3=False\n","DynamicLayer4=False\n","\n","DynamicLayer5=False\n","DynamicLayer6=False\n","DynamicLayer7=False\n","DynamicLayer8=False\n","#-----------------------------------------------------------------------------------------#\n","\n","\n","#------------------nn parameters----------------#\n","batch_size = 512 #hyper parameter\n","epochs = 32\n","random_seed = 1\n","opt_func = torch.optim.Adam\n","\n","lr = 0.001#start learning rate\n","max_lr = 0.01# maximun learning rate\n","grad_clip = 0.1\n","weight_decay = 1e-4\n","opt_func = torch.optim.Adam#optimizer\n","#----------------------------------------------#\n","\n","\n","\n","#Don't change:\n","#-------------#\n","val_size = 5000\n","#-------------#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RzaA6sAZdgxB"},"outputs":[],"source":["\n","def relu(input):\n","    x=input\n","    x[x<0]=0\n","    return x\n","\n","class RELU(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return relu(input) # simply apply already implemented SiLU\n","\n","def dynamic(input):\n","    x=input\n","    x[x<0]=0\n","\n","    if (checkCertainParams):\n","        x = x + (L0 / (Q0 * (x + I0) ** P0 + Z0)) - (L / (Q0 * (x - I0) ** P0 + Z0))\n","    else:\n","        x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    return x\n","\n","\n","class Dynamic(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return dynamic(input) # simply apply already implemented SiLU\n","activation_function=Dynamic()\n","def a_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer1:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L1\n","      QIN=Q1\n","      IIN=I1\n","      PIN=P1\n","      ZIN=Z1\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","\n","    return x\n","\n","class A_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return a_layer(input) # simply apply already implemented SiLU\n","\n","\n","\n","def b_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer2:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L2\n","      QIN=Q2\n","      IIN=I2\n","      PIN=P2\n","      ZIN=Z2\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","\n","    return x\n","\n","class B_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return b_layer(input) # simply apply already implemented SiLU\n","\n","\n","\n","\n","def c_layer(input):\n","    x=input\n","\n","    if DynamicLayer3:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L3\n","      QIN=Q3\n","      IIN=I3\n","      PIN=P3\n","      ZIN=Z3\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","    return x\n","\n","class C_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return c_layer(input) # simply apply already implemented SiLU\n","\n","\n","\n","def d_layer(input):\n","    x=input\n","    x[x<0]=0\n","\n","    if DynamicLayer4:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L4\n","      QIN=Q4\n","      IIN=I4\n","      PIN=P4\n","      ZIN=Z4\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","    return x\n","\n","class D_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return d_layer(input) # simply apply already implemented SiLU\n","\n","\n","\n","def e_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer5:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L5\n","      QIN=Q5\n","      IIN=I5\n","      PIN=P5\n","      ZIN=Z5\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","    return x\n","\n","class E_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return e_layer(input) # simply apply already implemented SiLU\n","\n","\n","\n","def f_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer6:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L6\n","      QIN=Q6\n","      IIN=I6\n","      PIN=P6\n","      ZIN=Z6\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","    return x\n","\n","class F_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return f_layer(input) # simply apply already implemented SiLU\n","\n","\n","def g_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer7:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L7\n","      QIN=Q7\n","      IIN=I7\n","      PIN=P7\n","      ZIN=Z7\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))\n","    return x\n","\n","class G_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return g_layer(input) # simply apply already implemented SiLU\n","\n","def h_layer(input):\n","    x=input\n","    x[x<0]=0\n","    if DynamicLayer8:\n","      x = x + (L / (Q * (x + I) ** P + Z)) - (L / (Q * (x - I) ** P + Z))\n","    else:\n","      LIN=L8\n","      QIN=Q8\n","      IIN=I8\n","      PIN=P8\n","      ZIN=Z8\n","      x = x + (LIN / (QIN * (x + IIN) ** PIN + ZIN)) - (LIN / (QIN * (x - IIN) ** PIN + ZIN))    \n","    return x\n","\n","class H_layer_activation(nn.Module):\n","    def __init__(self):\n","        super().__init__() # init the base class\n","    def forward(self, input):\n","        return h_layer(input) # simply apply already implemented SiLU\n","\n","def setParams(L_,Q_,P_,Z_,I_):\n","    global L\n","    global Q\n","    global P\n","    global Z\n","    global I\n","    L = L_\n","    Q = Q_\n","    P = P_\n","    Z = Z_\n","    I = I_\n","\n","    pass\n","\n","activation_function = RELU()\n","print(\"activation_function set\")\n","\n","#..............................end of our part...................................#"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7-Xj6KudwpQ"},"outputs":[],"source":["\"\"\"### Loading and Processing Dataset\"\"\"\n","\n","# Download the dataset\n","dataset_url = 'https://s3.amazonaws.com/fast-ai-imageclas/cifar100.tgz'\n","download_url(dataset_url, '.')\n","\n","#Extract from archive\n","with tarfile.open('./cifar100.tgz', 'r:gz') as tar:\n","  tar.extractall(path='./data')\n","\n","# Look into the data directory\n","data_dir = './data/cifar100'\n","folders = os.listdir(data_dir + \"/train\")\n","classes=[]\n","for folder in folders:\n","  classes+=os.listdir(data_dir + \"/train/\"+folder)\n","\n","\n","#Data transforms (normalization and data augmentation)\n","stats = ((0.4914, 0.4822, 0.4465),\n","         (0.2023, 0.1994, 0.2010))\n","\n","train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),\n","                         tt.RandomHorizontalFlip(),\n","                         tt.ToTensor(),\n","                         tt.Normalize(*stats, inplace=True)])\n","\n","valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n","\n","# PyTorch datasets\n","train_ds=[]\n","valid_ds=[]\n","for folder in folders:\n","  train_ds+=ImageFolder(data_dir+'/train/'+folder, train_tfms)\n","  valid_ds+=ImageFolder(data_dir+'/test/'+folder, valid_tfms)\n","\n","if not setAll :\n","    data_S = list(range(0, len(train_ds),int(1/train_percent)))\n","    test_S = list(range(0, len(valid_ds), int(1/val_percent)))\n","\n","    train_ds = torch.utils.data.Subset(train_ds, data_S)\n","    valid_ds = torch.utils.data.Subset(valid_ds, test_S)\n","    \n","\n","# Pytorch Data Loaders\n","train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=2, pin_memory=True)\n","\n","\"\"\"### Uploading on GPU\"\"\"\n","\n","### Using a GPU\n","def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)\n","\n","device = get_default_device()\n","print(device)\n","\n","train_dl = DeviceDataLoader(train_dl, device)\n","valid_dl = DeviceDataLoader(valid_dl, device)"]},{"cell_type":"markdown","metadata":{"id":"v2DhICEXqo2-"},"source":["# RESNET 9"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2imA4jcez78"},"outputs":[],"source":["def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n","\n","class ImageClassificationBase(nn.Module):\n","    def training_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                  # Generate predictions\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","\n","    def validation_step(self, batch):\n","        images, labels = batch\n","        out = self(images)                    # Generate predictions\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","\n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","\n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n","\n","\"\"\"Building our architecture:\"\"\"\n","\n","def conv_block(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              RELU()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_dynamic(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              Dynamic()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_A(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              A_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_B(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              B_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_C(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              C_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_D(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              D_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_E(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              E_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_F(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              F_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_G(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              G_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)\n","\n","def conv_block_H(in_channels, out_channels, pool=False):\n","    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","              nn.BatchNorm2d(out_channels),\n","              H_layer_activation()]\n","    if pool: layers.append(nn.MaxPool2d(2))\n","    return nn.Sequential(*layers)        \n","class ResNet9_dynamic(ImageClassificationBase):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.conv1 = conv_block_dynamic(in_channels, 64)\n","        self.conv2 = conv_block_dynamic(64, 128, pool=True)\n","        self.res1 = nn.Sequential(conv_block_dynamic(128, 128), conv_block_dynamic(128, 128))\n","        self.conv3 = conv_block_dynamic(128, 256, pool=True)\n","        self.conv4 = conv_block_dynamic(256, 512, pool=True)\n","        self.res2 = nn.Sequential(conv_block_dynamic(512, 512), conv_block_dynamic(512, 512))\n","\n","        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n","                                        nn.Flatten(),\n","                                        nn.Dropout(0.2),\n","                                        nn.Linear(512, num_classes))\n","\n","    def forward(self, xb):\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.res1(out) + out\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.res2(out) + out\n","        out = self.classifier(out)\n","        return out\n","class ResNet9_layers(ImageClassificationBase):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.conv1 = conv_block_A(in_channels, 64)\n","        self.conv2 = conv_block_B(64, 128, pool=True)\n","        self.res1 = nn.Sequential(conv_block_C(128, 128), conv_block_D(128, 128))\n","        self.conv3 = conv_block_E(128, 256, pool=True)\n","        self.conv4 = conv_block_F(256, 512, pool=True)\n","        self.res2 = nn.Sequential(conv_block_G(512, 512), conv_block_H(512, 512))\n","\n","        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n","                                        nn.Flatten(),\n","                                        nn.Dropout(0.2),\n","                                        nn.Linear(512, num_classes))\n","\n","    def forward(self, xb):\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.res1(out) + out\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.res2(out) + out\n","        out = self.classifier(out)\n","        return out\n","\n","class ResNet9_relu(ImageClassificationBase):\n","    def __init__(self, in_channels, num_classes):\n","        super().__init__()\n","        self.conv1 = conv_block(in_channels, 64)\n","        self.conv2 = conv_block(64, 128, pool=True)\n","        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n","        self.conv3 = conv_block(128, 256, pool=True)\n","        self.conv4 = conv_block(256, 512, pool=True)\n","        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n","\n","        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n","                                        nn.Flatten(),\n","                                        nn.Dropout(0.2),\n","                                        nn.Linear(512, num_classes))\n","\n","    def forward(self, xb):\n","        out = self.conv1(xb)\n","        out = self.conv2(out)\n","        out = self.res1(out) + out\n","        out = self.conv3(out)\n","        out = self.conv4(out)\n","        out = self.res2(out) + out\n","        out = self.classifier(out)\n","        return out\n","\n","\n","\"\"\"### Training the Model\n","The improvements in fit functions are:\n","1. Learning rate scheduling: Instead of using a fixed learning rate, we will use a learning rate scheduler, which will change the learning rate after every batch of training. We will use one cycle policy [1cycle policy](https://sgugger.github.io/the-1cycle-policy.html).\n","2. Weight Decay: A regularization technique which prevents the weights from becoming too large by adding an additional term to the loss function.\n","3. Gradient clipping: Apart from the layer weights and outputs, it also helpful to limit the values of gradients to a small range to prevent undesirable changes in parameters due to large gradient values\n","\n","\"\"\"\n","\n","@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n","                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n","    torch.cuda.empty_cache()\n","    history = []\n","\n","    # Set up cutom optimizer with weight decay\n","    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n","    # Set up one-cycle learning rate scheduler\n","    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n","                                                steps_per_epoch=len(train_loader))\n","\n","    for epoch in range(epochs):\n","        # Training Phase\n","        model.train()\n","        train_losses = []\n","        lrs = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            if grad_clip:\n","                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Record & update learning rate\n","            lrs.append(get_lr(optimizer))\n","            sched.step()\n","\n","        # Validation phase\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        result['lrs'] = lrs\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    return history\n","\n","# Evaluating the model before training"]},{"cell_type":"markdown","metadata":{"id":"V3wPaDxFqsrf"},"source":["TRAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQ4twFrQ1PDg"},"outputs":[],"source":["# Evaluating the model before training\n","\n","print(\"RELU on resnet9:\")\n","activation_function=RELU()\n","combinedScores=0\n","bestRELU=0\n","for i in range(HowManyTimesReRunRelu):\n","\n","  #model = to_device(ResNet9(3, 100), device)\n","  model = to_device(ResNet9_relu(3, 100), device)\n","  model\n","  history = [evaluate(model, valid_dl)]\n","  history\n","\n","  # Commented out IPython magic to ensure Python compatibility.\n","  # %%time\n","\n","  tic = time.time()\n","  history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n","                                grad_clip=grad_clip,\n","                                weight_decay=weight_decay,\n","                                opt_func=opt_func)\n","  toc = time.time()\n","  RELUscore=history[len(history)-1][\"val_acc\"]\n","  if addMaxRelu:\n","    if RELUscore>bestRELU:\n","      bestRELU=RELUscore\n","\n","  if addMean:\n","    combinedScores+=RELUscore\n","  print(\"time took:\", toc - tic,\"relu score is:\",RELUscore)\n","if addMaxRelu:\n","    print(\"RELU best score:\",bestRELU)\n","    RELUscore=bestRELU\n","\n","if addMean:  \n","  print(\"mean:\",combinedScores/HowManyTimesReRunRelu)\n","\n","\n","\n","if checkCertainParams:\n","  print(\"founded function on resnet9:\")\n","  maxCertainParams=0\n","  minCertainParams=1\n","  combinedScoresCertainParams=0\n","  for i in range(howManyTimeToCheckTheCertainParams):\n","    activation_function=Dynamic()\n","    model = to_device(ResNet9_layers(3, 100), device)\n","    model\n","    history = [evaluate(model, valid_dl)]\n","    history\n","\n","    tic = time.time()\n","    history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n","                                grad_clip=grad_clip,\n","                                weight_decay=weight_decay,\n","                                opt_func=opt_func)\n","    toc = time.time()\n","    score_cetrain=history[len(history)-1][\"val_acc\"]\n","    if addMaxToCertain:\n","      if score_cetrain> maxCertainParams:\n","        maxCertainParams=score_cetrain\n","      pass\n","    if addMinToCertain:\n","      if score_cetrain< minCertainParams:\n","        minCertainParams=score_cetrain\n","      pass\n","    if addMeanToCertain:   \n","      combinedScoresCertainParams+=score_cetrain\n","      pass \n","    print(\"time took:\", toc - tic)\n","  if addMaxToCertain:\n","    print(\"max score of the certain params is:\",maxCertainParams )\n","    pass \n","  if addMinToCertain:\n","    print(\"min score of the certain params is:\",minCertainParams )\n","    pass     \n","  if addMeanToCertain:\n","    if not howManyTimeToCheckTheCertainParams==0:\n","      print(\"mean score of the certain params is:\",combinedScoresCertainParams/howManyTimeToCheckTheCertainParams)\n","    pass        \n","\n","else:\n","  bestScore=0\n","  bestI=0\n","  bestQ=0\n","  bestL=0\n","  f = open(\"result.txt\", \"a\")\n","\n","  #Dynamic\n","  activation_function = Dynamic()\n","  I_=0\n","  Q_=0\n","  L_=0\n","  for i in range(SIZEI):\n","      for l in range(SIZEL):\n","          for q in range(SIZEQ):\n","              print(100*(q+l*SIZEQ+i*SIZEQ*SIZEL)/(SIZEI*SIZEL*SIZEQ),\"percent done\")\n","              model = to_device(ResNet9_layers(3, 100), device)\n","              to_device(model, device)\n","              history = [evaluate(model, valid_dl)]\n","\n","              I_=fromI+i*(toI-fromI)/SIZEI\n","              Q_=fromQ+q*(toQ-fromQ)/SIZEQ\n","              L_=fromL+l*(toL-fromL)/SIZEL\n","\n","              if(LogarithmINC):\n","                  I_=(((toI/fromI)**(1/SIZEI))**i)*fromI\n","                  Q_=(((toQ/fromQ)**(1/SIZEQ))**q)*fromQ\n","                  L_=(((toL/fromL)**(1/SIZEL))**l)*fromL\n","                  pass\n","              setParams(L_,Q_,P,Z,I_)\n","              tic = time.time()\n","              history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n","                                          grad_clip=grad_clip,\n","                                          weight_decay=weight_decay,\n","                                          opt_func=opt_func)\n","              toc = time.time()\n","              print(\"time took:\", toc - tic)\n","              score=history[len(history)-1][\"val_acc\"]\n","              f.write(\"score:\"+str(score)+\",\"+\"i:\"+str(I)+\",\"+\"q:\"+str(Q)+\",\"+\"l:\"+str(L)+\",\"+\"z:\"+str(Z)+\",\"+\"P:\"+str(P)+\",\"+\"val_percent:\"+str(val_percent)+\",\"+\n","                \"train_percent:\"+str(train_percent)+\",\"+\n","                \"batch:\"+str(batch_size)+\",\"+\"epochs:\"+str(epochs)+\",\"+\n","                \"lr:\"+str(lr)+\",\"+\"max_lr:\"+str(max_lr)+\",\"+\n","                \"grad_clip:\"+str(grad_clip)+\",\"+\n","                \"weight_decay:\"+str(weight_decay)+\",\"+str(history)+\"\\n\")\n","              if score>bestScore:\n","                  bestI = I_\n","                  bestQ = Q_\n","                  bestL = L_\n","                  bestScore=score\n","              print(score,\"with i=\",I_,\"q=\",Q_,\"l=\",L_)\n","\n","\n","  f.close()\n","  print(\"best score:\",bestScore,\"with i=\",bestI,\"q=\",bestQ,\"l=\",bestL,\"relu best score was:\",RELUscore)"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"layers_unique_parameters_cifar100.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}